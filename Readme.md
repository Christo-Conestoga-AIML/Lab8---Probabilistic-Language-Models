# Probabilistic Language Models

This project was created for a team-based NLP workshop focused on understanding and implementing basic language modeling techniques. We built an NLP pipeline, created inverted indexes, and developed unigram and bigram models to estimate the probability of sentences.

## Project Objective

To build a small but complete NLP pipeline using:
- Tokenization and text preprocessing
- Inverted index creation
- Unigram and bigram models (Maximum Likelihood Estimation)
- Add-one (Laplace) smoothing
- Sentence probability estimation

This helps us understand how language models work under the hood.

## Dataset Description

We used the **Reuters Corpus** from the NLTK library. It contains thousands of news articles covering many topics like business, sports, and politics. This corpus is ideal for training language models because it has a rich vocabulary and diverse topics.

## Folder Structure

- `NLP_Workshop_Probabilistic_Language_Models_final.ipynb`: Main notebook containing all code and explanations
- `README.md`: This file

## Team Members

- [Your Name 1]
- [Your Name 2]
- [Your Name 3]

## Dataset Link and License

- Dataset: [Reuters Corpus from NLTK](https://www.nltk.org/nltk_data/)
- License: Provided for academic/research use via the NLTK package

## How to Run

1. Clone the repository
2. Open the notebook using Jupyter or VS Code
3. Run each cell in order to see the full pipeline in action

## Talking Points

We added labeled “Talking Points” throughout the notebook to help explain key ideas and demonstrate our understanding of the models.

