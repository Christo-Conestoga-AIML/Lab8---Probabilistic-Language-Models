# Probabilistic Language Models

This project was created for a team-based NLP workshop focused on understanding and implementing basic language modeling techniques. We built an NLP pipeline, created inverted indexes, and developed unigram and bigram models to estimate the probability of sentences.

## Project Objective

To build a small but complete NLP pipeline using:
- Tokenization and text preprocessing
- Inverted index creation
- Unigram and bigram models (Maximum Likelihood Estimation)
- Add-one (Laplace) smoothing
- Sentence probability estimation

This helps us understand how language models work under the hood.

## Dataset Description

We used the **Reuters Corpus** from the NLTK library. It contains thousands of news articles covering many topics like business, sports, and politics. This corpus is ideal for training language models because it has a rich vocabulary and diverse topics.

## Folder Structure

- `NLP_Workshop_Probabilistic_Language_Models.ipynb`: Main notebook containing all code and explanations
- `README.md`: This file

## Team Members

1. Fasalu Rahman Kottaparambu 8991782
2. Christo Pananjickal Baby 8989796
3. Srinu Babu Rai 8994032

## Dataset Link and License

- Dataset: [UET_Administrative_Chatbot](https://www.kaggle.com/datasets/ashirmaqbool1611/uet-administrative-chatbot)
- License: Unknown or not present on kaggle

## How to Run

1. Clone the repository
2. Open the notebook using Jupyter or VS Code
3. Install requirements.txt
4. Run each cell in order to see the full pipeline in action

## Talking Points

We added labeled “Talking Points” throughout the notebook to help explain key ideas and demonstrate our understanding of the models.

